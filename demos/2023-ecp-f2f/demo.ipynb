{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfba5cdb-f769-4d04-90cd-c8ec30be08a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Demonstrator 2023-ecp-f2f\n",
    "\n",
    "Refer to `README.md` for installation instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc2771-56c5-4980-a272-820c96d0b825",
   "metadata": {},
   "source": [
    "## 0. Configuration\n",
    "\n",
    "Let's start off by sourcing secrets and defining the locations of the various required services:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad8b4b-f59e-4f65-bf0d-8645309ee4ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# source secrets\n",
    "source .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3869795-9bf5-4642-ba13-020c05e26170",
   "metadata": {},
   "source": [
    "#### **List TES instances**\n",
    "\n",
    "Let's see what TES instances we have defined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fe37d-05cc-4a54-8a67-3a6c01e5ad8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unset TES_INSTANCES\n",
    "declare -A TES_INSTANCES\n",
    "while IFS=',' read -r KEY URL; do\n",
    "    TES_INSTANCES[\"$KEY\"]=$URL\n",
    "done < .tes_instances\n",
    "\n",
    "for KEY in \"${!TES_INSTANCES[@]}\"; do\n",
    "    echo \"$KEY: ${TES_INSTANCES[$KEY]}\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e23aef-442c-4bc9-ad40-d8e95d07e720",
   "metadata": {},
   "source": [
    "If you are running the demo on the ELIXIR Cloud infrastructure, the demo\n",
    "makes use of the following TES and cloud storage deployments.\n",
    "\n",
    "**Figure 0. ELIXIR Cloud deployments.**\n",
    "\n",
    "![Figure 0](images/figure_0.svg)\n",
    "\n",
    "> This setup will of course differ if you deployed your own nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e3335-a343-4504-9799-c8d5f1b5a4d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Executing tasks via the GA4GH TES API\n",
    "\n",
    "In this section, we will use both the shell and a dedicated Python library to\n",
    "send tasks to the defined TES instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6780fb81-0d37-43fb-8aa4-a2a3a80069b2",
   "metadata": {},
   "source": [
    "### Using the shell\n",
    "\n",
    "Here, we will use the `curl` library to send requests to the TES APIs. It\n",
    "should be easy to adapt the calls for use with other tools, such as Postman or\n",
    "your favorite progamming language's HTTP request libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38fe7c4-d146-479b-afd9-a7716a421e4a",
   "metadata": {},
   "source": [
    "#### **Running a minimal task**\n",
    "\n",
    "Now we will submit a very simple task to each of these instances (Figure 1A).\n",
    "\n",
    "**Figure 1A. Executing a minimal task.**\n",
    "\n",
    "![Figure 1A](images/figure_1A.svg)\n",
    "\n",
    "The task we use here defines no inputs and outputs, so we do not need to read\n",
    "from or write to any storage instances.\n",
    "\n",
    "The payload for the task needs to be provided in JSON format. Nicely formatted,\n",
    "it looks like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"executors\": [\n",
    "    {\n",
    "      \"image\": \"alpine\",\n",
    "      \"command\": [\n",
    "        \"echo\",\n",
    "        \"hello\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "With these instructions, we are asking the TES instances to execute the command\n",
    "`echo hello` in (the default version of) an Alpine Linux container.\n",
    "\n",
    "Let's minify the JSON payload and assign it to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010e6343-a8b8-44ee-afb5-c3ce25f2293e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAYLOAD='{\"executors\":[{\"image\":\"alpine\",\"command\":[\"echo\",\"hello\"]}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72916da9-1de2-4665-8be6-24f2636af0fe",
   "metadata": {},
   "source": [
    "Now we are ready to submit the tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8186fe6e-ecf7-4856-b421-7a3c9aa45e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unset TASKS\n",
    "declare -A TASKS\n",
    "for KEY in \"${!TES_INSTANCES[@]}\"; do\n",
    "    TES=\"${TES_INSTANCES[$KEY]}\"\n",
    "    echo \"Submitting task to $KEY ($TES)...\"\n",
    "    TASK_ID=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"POST\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            --data \"$PAYLOAD\" \\\n",
    "            \"${TES%/}/v1/tasks\" | \\\n",
    "        jq \".id\" - | \\\n",
    "        tr -d '\"'\n",
    "    )\n",
    "    if [ $TASK_ID == \"null\" ]; then\n",
    "        echo \"FAILED\"\n",
    "    else\n",
    "        echo \"Task ID: $TASK_ID\"\n",
    "        TASKS[\"$TASK_ID\"]=\"$TES\"\n",
    "    fi\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf53cf-de1e-43e4-920d-ca52dff6b29e",
   "metadata": {},
   "source": [
    "Let's see how the execution of successfully submitted tasks is progressing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce17adf7-88a6-4738-9f1f-b6d243010fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for TASK_ID in \"${!TASKS[@]}\"; do\n",
    "    TES=\"${TASKS[$TASK_ID]}\"\n",
    "    echo \"Checking state of task '$TASK_ID' ($TES)...\"\n",
    "    RESPONSE=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"GET\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            \"${TES%/}/v1/tasks/${TASK_ID}\" \\\n",
    "    )\n",
    "    echo -n \"Task State: \"\n",
    "    echo $RESPONSE | jq \".state\"\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e120ede-9fd7-4cee-b58e-ef7a1b1fd7e7",
   "metadata": {},
   "source": [
    "#### **Running a task with inputs and outputs**\n",
    "\n",
    "Let's try a little more realistic task with an input (from the web) and an\n",
    "output (written to an FTP instance), as depicted in Figure 1B.\n",
    "\n",
    "**Figure 1B. Data flow for TES execution.** TES calls are as in Figure 1A and\n",
    "have been omitted for clarity.\n",
    "\n",
    "![Figure 1B](images/figure_1B.svg)\n",
    "\n",
    "We define the following payload:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"name\": \"md5sum\",\n",
    "  \"description\": \"calculate md5sum of input file and write to output file\",\n",
    "  \"tags\": {\n",
    "    \"project\": \"2023-ecp-f2f Demonstrator\",\n",
    "    \"project_owner\": \"ELIXIR Cloud & AAI\"\n",
    "  },\n",
    "  \"executors\": [\n",
    "    {\n",
    "      \"command\": [\n",
    "        \"md5sum\",\n",
    "        \"/data/input\"\n",
    "      ],\n",
    "      \"image\": \"alpine\",\n",
    "      \"stdout\": \"/data/output\",\n",
    "      \"workdir\": \"/data\"\n",
    "    }\n",
    "  ],\n",
    "  \"inputs\": [\n",
    "    {\n",
    "      \"url\": \"{{INPUT_FILE}}\",\n",
    "      \"path\": \"/data/input\"\n",
    "    }\n",
    "  ],\n",
    "  \"outputs\": [\n",
    "    {\n",
    "      \"path\": \"/data/output\",\n",
    "      \"url\": \"{{FTP_INSTANCE}}/2023-ecp-f2f/md5sum\",\n",
    "      \"type\": \"FILE\"\n",
    "    }\n",
    "  ],\n",
    "  \"resources\": {\n",
    "    \"cpu_cores\": 1,\n",
    "    \"disk_gb\": 1,\n",
    "    \"preemptible\": false,\n",
    "    \"ram_gb\": 1\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "As you can see, here we determine the MD5 sum of an input file, write it to an\n",
    "output file inside the container, and finally copy it over to an FTP server.\n",
    "\n",
    "Let's minify that and replace the placeholders `{{INPUT_FILE}}` and\n",
    "`{{FTP_INSTANCE}}` with some actual values.\n",
    "\n",
    "> Note that because Funnel does currently only allow [passing FTP storage\n",
    "> credentials via the FTP\n",
    "> URL](https://ohsu-comp-bio.github.io/funnel/docs/storage/ftp/) and TESK does\n",
    "> not support FTP URLs with credentials, we need to use different payloads for\n",
    "> the two services!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1454c4e4-7ed2-42d3-aa38-379788d120c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAYLOAD_RAW='{\"name\":\"md5sum\",\"description\":\"calculate md5sum of input file and write to output file\",\"tags\":{\"project\":\"2023-ecp-f2f Demonstrator\",\"project_owner\":\"ELIXIR Cloud & AAI\"},\"executors\":[{\"command\":[\"md5sum\",\"/data/input\"],\"image\":\"alpine\",\"stdout\":\"/data/output\",\"workdir\":\"/data\"}],\"inputs\":[{\"url\":\"{{INPUT_FILE}}\",\"path\":\"/data/input\"}],\"outputs\":[{\"path\":\"/data/output\",\"url\":\"{{FTP_INSTANCE}}/2023-ecp-f2f/md5sum\",\"type\":\"FILE\"}],\"resources\":{\"cpu_cores\":1,\"disk_gb\":1,\"preemptible\":false,\"ram_gb\":1}}'\n",
    "PAYLOAD_TMP=$(sed 's#{{INPUT_FILE}}#https://raw.githubusercontent.com/elixir-cloud-aai/elixir-cloud-demos/df5be391faf992ebcd5ec2b2aad581c99de26101/LICENSE#' <<< $PAYLOAD_RAW)\n",
    "PAYLOAD_TESK=$(sed \"s|{{FTP_INSTANCE}}|${FTP_INSTANCE%/}|\" <<< $PAYLOAD_TMP)\n",
    "PAYLOAD_FUNNEL=$(sed \"s|ftp://|ftp://${FTP_USER}:${FTP_PASSWORD}@|g\" <<< $PAYLOAD_TESK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e6d2df-7c63-45b4-95c3-cff841d3bb8b",
   "metadata": {},
   "source": [
    "Let's submit as before (but setting the payload according to the service):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd3294-f65d-47ee-8f97-099ef44fc32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unset TASKS\n",
    "declare -A TASKS\n",
    "for KEY in \"${!TES_INSTANCES[@]}\"; do\n",
    "    TES=\"${TES_INSTANCES[$KEY]}\"\n",
    "    if [[ $KEY =~ \"Funnel\" ]]; then\n",
    "        PAYLOAD=\"$PAYLOAD_FUNNEL\"\n",
    "    else\n",
    "        PAYLOAD=\"$PAYLOAD_TESK\"\n",
    "    fi\n",
    "    echo \"Submitting task to $KEY ($TES)...\"\n",
    "    TASK_ID=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"POST\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            --data \"$PAYLOAD\" \\\n",
    "            \"${TES%/}/v1/tasks\" | \\\n",
    "        jq \".id\" - | \\\n",
    "        tr -d '\"'\n",
    "    )\n",
    "    if [ $TASK_ID == \"null\" ]; then\n",
    "        echo \"FAILED\"\n",
    "    else\n",
    "        echo \"Task ID: $TASK_ID\"\n",
    "        TASKS[\"$TASK_ID\"]=\"$TES\"\n",
    "    fi\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cdfb98-7a8c-4cab-9270-33ec79d4549c",
   "metadata": {},
   "source": [
    "And check the states, but with a lot more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21dbec3-ff97-454f-921c-39a971f14f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIEW=BASIC\n",
    "for TASK_ID in \"${!TASKS[@]}\"; do\n",
    "    TES=\"${TASKS[$TASK_ID]}\"\n",
    "    echo \"Checking state of task '$TASK_ID' ($TES)...\"\n",
    "    RESPONSE=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"GET\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            \"${TES%/}/v1/tasks/${TASK_ID}?view=${VIEW}\" | \\\n",
    "        sed \"s/${FTP_USER}:${FTP_PASSWORD}@//g\"  # remove FTP credentials from logs\n",
    "    )\n",
    "    echo $RESPONSE | jq \".\"\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4127a9-c835-4611-a3f7-d19f9269345c",
   "metadata": {},
   "source": [
    "#### **Other TES operations**\n",
    "\n",
    "We have seen how we can submit tasks and get summary or detailed information on\n",
    "individual tasks.\n",
    "\n",
    "The full list of currently supported operations is:\n",
    "\n",
    "| HTTP Method | Endpoint | Description |\n",
    "| --- | --- | --- |\n",
    "| GET | `/service-info` | Fetch information about the service and its optional capabilities |\n",
    "| POST | `/tasks` | Create a task |\n",
    "| GET | `/tasks` | Fetch a list of all tasks |\n",
    "| GET | `/tasks/{task_id}` | Fetch details about a specific task |\n",
    "| POST | `/tasks/{task_id}:cancel` | Cancel a task |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6663d42f-5cf3-4d29-b8b9-e6bae65d1d2b",
   "metadata": {},
   "source": [
    "### Using the `py-tes` Python library\n",
    "\n",
    "In this section, we are submitting the simple task from above using the Python\n",
    "TES client `py-tes` (Figure 1C).\n",
    "\n",
    "**Figure 1C. Submitting tasks via the Python library.**\n",
    "\n",
    "![Figure 1C](images/figure_1C.svg)\n",
    "\n",
    "Usage is simple (from the [`py-tes` repository](https://github.com/ohsu-comp-bio/py-tes)):\n",
    "\n",
    "```python\n",
    "import tes\n",
    "\n",
    "task = tes.Task(\n",
    "    executors=[\n",
    "        tes.Executor(\n",
    "            image=\"alpine\",\n",
    "            command=[\"echo\", \"hello\"]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "cli = tes.HTTPClient(\"http://funnel.example.com\", timeout=5)\n",
    "task_id = cli.create_task(task)\n",
    "res = cli.get_task(task_id)\n",
    "```\n",
    "\n",
    "To do so, we will execute the Python script `task_submission.py`, which\n",
    "triggers the execution of our minimal task on the available TES instances and\n",
    "then checks the task state periodically until all tasks are in a finished state\n",
    "(one of `COMPLETE`, `EXECUTOR_ERROR`, `SYSTEM_ERROR`, or `CANCELLED`) or six\n",
    "checks (whichever occurs first).\n",
    "\n",
    "Here are the contents of the script:\n",
    "\n",
    "```python\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"Demonstrate task submission via py-tes.\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import sys\n",
    "from time import sleep\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import tes\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.getLogger(\"tes\").setLevel(logging.WARNING) \n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    # set up logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format= '[%(asctime)s] %(levelname)s - %(message)s',\n",
    "        datefmt='%H:%M:%S'\n",
    "    )\n",
    "\n",
    "    # import TES instances\n",
    "    _file=Path(\".tes_instances\")\n",
    "    LOGGER.info(f\"Importing TES instances from file {str(_file)}\")\n",
    "    try:\n",
    "        TES_INSTANCES = csv_to_dict(_file=\".tes_instances\")\n",
    "    except FileNotFoundError:\n",
    "        LOGGER.critical(f\"No TES instances defined. Aborting.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # list TES instances\n",
    "    LOGGER.info(f\"Available TES instances:\")\n",
    "    for idx, (key, url) in enumerate(TES_INSTANCES.items()):\n",
    "        LOGGER.info(f\"({idx + 1}) {key}: {url}\")\n",
    "\n",
    "    # set task payload\n",
    "    LOGGER.info(f\"Setting task payload...\")\n",
    "    task=tes.Task(\n",
    "        executors=[\n",
    "            tes.Executor(\n",
    "                image=\"alpine\",\n",
    "                command=[\"echo\", \"hello\"]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # submit tasks\n",
    "    task_ids: Dict[str, str] = {}\n",
    "    for key, url in TES_INSTANCES.items():\n",
    "        LOGGER.info(f\"Submitting task to {key} ({url})...\")\n",
    "        try:\n",
    "            task_id = submit_task(task=task, url=url)\n",
    "        except requests.exceptions.HTTPError as exc:\n",
    "            LOGGER.warning(f\"FAILED: {exc}\")\n",
    "            continue\n",
    "        task_ids[task_id] = url\n",
    "        LOGGER.info(f\"Task ID: {task_id}\")\n",
    "\n",
    "    # check task states periodically until all tasks finished\n",
    "    task_states: Dict = dict.fromkeys(task_ids, \"UNKNOWN\")\n",
    "    FINAL_STATES = [\"COMPLETE\", \"EXECUTOR_ERROR\", \"SYSTEM_ERROR\", \"CANCELLED\"]\n",
    "    sleep_time=5\n",
    "    repeats=4\n",
    "    for _ in range(repeats):\n",
    "        LOGGER.info(f\"Waiting for {sleep_time} seconds...\")\n",
    "        sleep(sleep_time)\n",
    "        LOGGER.info(f\"Checking states of all tasks...\")\n",
    "        for task_id, url in task_ids.items():\n",
    "            LOGGER.info(f\"Checking state of task '{task_id}' ({url})...\")\n",
    "            task_state = get_task_state(task_id=task_id, url=url)\n",
    "            task_states[task_id] = task_state\n",
    "            LOGGER.info(f\"Task state: {task_state}\")\n",
    "        if all(state in FINAL_STATES for _, state in task_states.items()):\n",
    "            LOGGER.info(f\"All tasks concluded.\")\n",
    "            break\n",
    "    else:\n",
    "        LOGGER.warning(\n",
    "            f\"Checks timed out while one or more tasks were still processing.\"\n",
    "        )\n",
    "    LOGGER.info(\"Done\")\n",
    "\n",
    "\n",
    "def csv_to_dict(_file: str) -> Dict:\n",
    "    \"\"\"Create dictionary from first two fields of a CSV file.\n",
    "\n",
    "    Any other columns are ignored.\n",
    "\n",
    "    Args:\n",
    "        _file: Path to file with associative array contents.\n",
    "\n",
    "    Returns:\n",
    "        Bash associative array contents as dictionary.\n",
    "    \"\"\"\n",
    "    _dict: Dict = {}\n",
    "    with open(_file, \"r\") as _f:\n",
    "        for line in _f:\n",
    "            line_split = line.strip().split(\",\", maxsplit=2)\n",
    "            _dict[line_split[0]] = line_split[1]\n",
    "    return _dict \n",
    "\n",
    "\n",
    "def submit_task(\n",
    "    task: tes.Task,\n",
    "    url: str,\n",
    "    timeout: int = 5,\n",
    "    user: Optional[str] = os.environ.get('FUNNEL_SERVER_USER'),\n",
    "    password: Optional[str] = os.environ.get('FUNNEL_SERVER_PASSWORD'),\n",
    ") -> str:\n",
    "    \"\"\"Submit task to TES instance.\n",
    "\n",
    "    Args:\n",
    "        task: Task to submit.\n",
    "        url: TES instance URL.\n",
    "        timeout: Timeout in seconds.\n",
    "        user: Username for authentication.\n",
    "        password: Password for authentication.\n",
    "\n",
    "    Returns:\n",
    "        Identifier of submitted task.\n",
    "    \"\"\"\n",
    "    cli = tes.HTTPClient(url, timeout=timeout, user=user, password=password)\n",
    "    return cli.create_task(task=task)\n",
    "\n",
    "\n",
    "def get_task_state(\n",
    "    task_id: str,\n",
    "    url: str,\n",
    "    timeout: int = 5,\n",
    "    user: Optional[str] = os.environ.get('FUNNEL_SERVER_USER'),\n",
    "    password: Optional[str] = os.environ.get('FUNNEL_SERVER_PASSWORD'),\n",
    ") -> str:\n",
    "    \"\"\"Check state of task.\n",
    "\n",
    "    Args:\n",
    "        task_id: Identifier of task.\n",
    "        url: TES instance URL.\n",
    "        timeout: Timeout in seconds.\n",
    "        user: Username for authentication.\n",
    "        password: Password for authentication.\n",
    "\n",
    "    Returns:\n",
    "        State of task.\n",
    "    \"\"\"\n",
    "    cli = tes.HTTPClient(url, timeout=timeout, user=user, password=password)\n",
    "    return cli.get_task(task_id=task_id).state\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n",
    "\n",
    "Now let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d45ea-2f1f-477a-82b2-ff091988a768",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "./task_submission.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff82e60-5f4c-4457-b952-a9fe12f3b2bb",
   "metadata": {},
   "source": [
    "Note that the `py-tes` library is used in various TES tools, including the\n",
    "[proTES](https://github.com/elixir-cloud-aai/proTES) gateway (see below) and\n",
    "the [cwl-tes](https://github.com/uniqueg/cwl-tes) and\n",
    "[Snakemake](https://snakemake.readthedocs.io/) workflow engines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf96900-7886-458f-9510-77a8406a6fb0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Sending tasks to TES gateway\n",
    "\n",
    "In this section, instead of sending tasks directly to the TES instances, we\n",
    "will send tasks to the [proTES](https://github.com/elixir-cloud-aai/proTES)\n",
    "gateway instead. The proTES, which acts both as a TES server and client, will\n",
    "then relay incoming tasks to the TES instances it knows about (this currently\n",
    "needs to be configured when deploying).\n",
    "\n",
    "To determine to which TES instance proTES relays a given task, two distribution\n",
    "logics have been implemented:\n",
    "- Random distribution\n",
    "- Distance-based distribution\n",
    "\n",
    "We will start off with randomly distributing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e818e6-6519-4751-9194-a59ec3e7fc7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random task distribution\n",
    "\n",
    "In the random task distribution, for each incoming task, the known TES\n",
    "instances are randonly sorted and arranged into a ranked list. proTES will\n",
    "then attempt to forward the TES request to the first item in the list. If the\n",
    "request succeeds and a task identifier is returned, proTES will return its own\n",
    "task identifier to the client. If a task submission fails (e.g., because the\n",
    "TES instance is down), proTES will attempt to submit the task to the next TES\n",
    "instance in the list.\n",
    "\n",
    "After submitting a task, proTES will continue monitoring the execution of the\n",
    "task on the remote service in the background. In this way, the client can query\n",
    "the task status conveniently via proTES. Indeed, proTES is a complete TES API\n",
    "implementation, so the task list, task cancellation and service info endpoints\n",
    "are available as well. Thus, from the client's point of view, proTES is\n",
    "indistinguishable from a \"real\" TES instance, i.e., one that does the actual\n",
    "computation of the task as requested.\n",
    "\n",
    "For the random task distribution, we will use the minimal task definition from\n",
    "before, as it is sufficient to demonstrate the principle. Also, the random task\n",
    "distribution middleware is chosen by default, when proTES receives tasks\n",
    "without inputs. The service calls are depicted in Figure 2A.\n",
    "\n",
    "**Figure 2A. Task distribution via the proTES gateway.**\n",
    "\n",
    "![Figure 2A](images/figure_2A.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5e07e3-3e40-4816-ae54-8811366cdb6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAYLOAD='{\"executors\":[{\"image\":\"alpine\",\"command\":[\"echo\",\"hello\"]}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df10633a-3827-4e77-b01a-d98c5a7a3f4b",
   "metadata": {},
   "source": [
    "Now let's set the gateway as the TES instance once and for all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63be9437-3119-409d-94de-c97760f85b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TES=\"$TES_GATEWAY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b78ac5-5119-4a2f-ba56-eb4d55b377d2",
   "metadata": {},
   "source": [
    "Okay. Let's start off with a single task submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b061e06-5538-4206-8deb-c88284e3e332",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "echo \"Submitting task to TES gateway ($TES)...\"\n",
    "TASK_ID=$( \\\n",
    "    curl \\\n",
    "        --silent \\\n",
    "        --request \"POST\" \\\n",
    "        --header \"accept: application/json\" \\\n",
    "        --header \"Content-Type: application/json\" \\\n",
    "        --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "        --data \"$PAYLOAD\" \\\n",
    "        \"${TES%/}/ga4gh/tes/v1/tasks\" | \\\n",
    "    jq \".id\" - | \\\n",
    "    tr -d '\"'\n",
    ")\n",
    "if [ $TASK_ID == \"null\" ]; then\n",
    "    echo \"FAILED\"\n",
    "else\n",
    "    echo \"Task ID: $TASK_ID\"\n",
    "fi\n",
    "echo \"================================================================================\"\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a62c51-9e47-4d65-aa13-dddf1b081228",
   "metadata": {},
   "source": [
    "And let's inspect the logs in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abcd0a-0e33-4445-ab9c-1c741b61625b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VIEW=FULL\n",
    "echo \"Checking state of task '$TASK_ID' ($TES)...\"\n",
    "RESPONSE=$( \\\n",
    "    curl \\\n",
    "        --silent \\\n",
    "        --request \"GET\" \\\n",
    "        --header \"accept: application/json\" \\\n",
    "        --header \"Content-Type: application/json\" \\\n",
    "        --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "        \"${TES%/}/ga4gh/tes/v1/tasks/${TASK_ID}?view=${VIEW}\" \\\n",
    ")\n",
    "echo $RESPONSE | jq \".\"\n",
    "echo \"================================================================================\"\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339bfe-0ff2-41e8-91ae-1da86f806be5",
   "metadata": {},
   "source": [
    "Now, to ensure that we submit to different TES instances, let's send off a\n",
    "couple of task requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac3b4b7-e951-4b6a-aeba-b85f926759a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_TASKS=$(( 2 * ${#TES_INSTANCES[@]} ))\n",
    "unset TASKS\n",
    "declare -A TASKS\n",
    "for INDEX in $(seq $N_TASKS); do\n",
    "    echo \"Submitting task to TES gateway ($TES)...\"\n",
    "    TASK_ID=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"POST\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            --data \"$PAYLOAD\" \\\n",
    "            \"${TES%/}/ga4gh/tes/v1/tasks\" | \\\n",
    "        jq \".id\" - | \\\n",
    "        tr -d '\"'\n",
    "    )\n",
    "    if [ $TASK_ID == \"null\" ]; then\n",
    "        echo \"FAILED\"\n",
    "    else\n",
    "        echo \"Task ID: $TASK_ID\"\n",
    "        TASKS[\"$TASK_ID\"]=\"$TES\"\n",
    "    fi\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c123dd1-eedd-46c4-bcf6-adda3a18968a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for TASK_ID in \"${!TASKS[@]}\"; do\n",
    "    echo \"Checking state of task '$TASK_ID' ($TES)...\"\n",
    "    RESPONSE=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"GET\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            \"${TES%/}/ga4gh/tes/v1/tasks/${TASK_ID}?view=BASIC\"\\\n",
    "    )\n",
    "    echo -n \"Task State: \"\n",
    "    echo $RESPONSE | jq \".state\"\n",
    "    echo -n \"Executed at: \"\n",
    "    echo $RESPONSE | jq '.logs[0].metadata.forwarded_to.url'\n",
    "    echo -n \"External task ID: \"\n",
    "    echo $RESPONSE | jq '.logs[0].metadata.forwarded_to.id'\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5ee0d5-1143-4217-bf5e-6d4d7655ce4e",
   "metadata": {},
   "source": [
    "### Distance-based task distribution\n",
    "\n",
    "With this distribution logic, proTES determines the approximate locations of\n",
    "all known TES instances and all input files for a task via their IP\n",
    "geolocations. For each TES instance, it then calculates the geographic distance\n",
    "to each input file and sums up the total distance for all files. Finally, it\n",
    "rank orders the available TES instances by total distance in ascending order.\n",
    "proTES then tries to forward the incoming task to the top entry of the resuling\n",
    "list, as described for the random distribution logic.\n",
    "\n",
    "In this way, the total geographic distance that files (but not bytes, as the\n",
    "input files sizes are unknown!) have to travel is minimized.\n",
    "\n",
    "Of course we will need a task with inputs. We will use our previous task again,\n",
    "as one input file should be sufficient to prove our point. It is also easier to\n",
    "check whether the distribution logic chose the right TES. To make it a bit more\n",
    "interesting, we will run the task several times, but each attempt with an input\n",
    "file at different location. The exercise is visualized in Figure 2B.\n",
    "\n",
    "**Figure 2B. Data flow for distance-based task distribution.** Service calls\n",
    "are as in Figure 2A and have been omitted for clarity.\n",
    "\n",
    "![Figure 2B](images/figure_2B.svg)\n",
    "\n",
    "Let's define our payload as before, but let's create one version per input\n",
    "file, with files hosted in the Czech Republic, Finland, Greece and Switzerland:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def6c3b-dfaa-452c-9748-03e4b212ecfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PAYLOAD_RAW='{\"name\":\"md5sum\",\"description\":\"calculate md5sum of input file and write to output file\",\"tags\":{\"project\":\"2023-ecp-f2f Demonstrator\",\"project_owner\":\"ELIXIR Cloud & AAI\"},\"executors\":[{\"command\":[\"md5sum\",\"/data/input\"],\"image\":\"alpine\",\"stdout\":\"/data/output\",\"workdir\":\"/data\"}],\"inputs\":[{\"url\":\"{{INPUT_FILE}}\",\"path\":\"/data/input\",\"type\":\"FILE\"}],\"outputs\":[{\"path\":\"/data/output\",\"url\":\"{{FTP_INSTANCE}}/2023-ecp-f2f/md5sum\",\"type\":\"FILE\"}],\"resources\":{\"cpu_cores\":1,\"disk_gb\":1,\"preemptible\":false,\"ram_gb\":1}}'\n",
    "PAYLOAD_TMP_1=$(sed \"s|{{FTP_INSTANCE}}|${FTP_INSTANCE%/}|\" <<< $PAYLOAD_RAW)\n",
    "PAYLOAD_TMP_2=$(sed \"s|ftp://|ftp://${FTP_USER}:${FTP_PASSWORD}@|g\" <<< $PAYLOAD_TMP_1)\n",
    "\n",
    "unset INPUTS\n",
    "unset PAYLOADS\n",
    "declare -A INPUTS\n",
    "declare -A PAYLOADS\n",
    "while IFS=',' read -r KEY URL; do\n",
    "    INPUTS[\"$KEY\"]=$URL\n",
    "    PAYLOADS[\"$KEY\"]=$(sed \"s#{{INPUT_FILE}}#${URL}#\" <<< $PAYLOAD_TMP_2)\n",
    "done < .inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bfc06b-938e-423e-be44-18c79ffb62b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unset TASKS\n",
    "declare -A TASKS\n",
    "for LOCATION in \"${!PAYLOADS[@]}\"; do\n",
    "    if [ \"$LOCATION\" == \"workflow\" ]; then\n",
    "        continue\n",
    "    fi\n",
    "    PAYLOAD=\"${PAYLOADS[$LOCATION]}\"\n",
    "    echo \"Submitting task with input data in $LOCATION ($TES)...\"\n",
    "    TASK_ID=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"POST\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            --data \"$PAYLOAD\" \\\n",
    "            \"${TES%/}/ga4gh/tes/v1/tasks\" | \\\n",
    "        jq \".id\" - | \\\n",
    "        tr -d '\"'\n",
    "    )\n",
    "    if [ $TASK_ID == \"null\" ]; then\n",
    "        echo \"FAILED\"\n",
    "    else\n",
    "        echo \"Task ID: $TASK_ID\"\n",
    "        TASKS[\"$TASK_ID\"]=\"$LOCATION\"\n",
    "    fi\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449a0336-c6bc-4378-a14b-7403ee9a4e83",
   "metadata": {},
   "source": [
    "It took a little while to do all the IP geolocation lookups. Let's see if it\n",
    "was worth the wait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf6d2a-a626-4def-93a9-a9b5291a42ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for TASK_ID in \"${!TASKS[@]}\"; do\n",
    "    LOCATION=\"${TASKS[$TASK_ID]}\"\n",
    "    echo \"Checking state of task '$TASK_ID' (input data in: $LOCATION)...\"\n",
    "    RESPONSE=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"GET\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            \"${TES%/}/ga4gh/tes/v1/tasks/${TASK_ID}?view=BASIC\"\\\n",
    "    )\n",
    "    echo -n \"Task State: \"\n",
    "    echo $RESPONSE | jq \".state\"\n",
    "    echo -n \"Executed at: \"\n",
    "    echo $RESPONSE | jq '.logs[0].metadata.forwarded_to.url'\n",
    "    echo -n \"External task ID: \"\n",
    "    echo $RESPONSE | jq '.logs[0].metadata.forwarded_to.id'\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9aed58-856c-40a8-8217-937e2f448525",
   "metadata": {},
   "source": [
    "You should see that each task was computed in the TES instance that is\n",
    "geographically closest to the location of the particular input data files.\n",
    "Nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeba35b-b7e5-485e-8421-897a39916d2f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. Executing workflows via the TES network\n",
    "\n",
    "In this section, we will demonstrate how a workflow engine with a TES backend\n",
    "can make use of the TES gateway to execute workflows on a network of TES\n",
    "instances.\n",
    "\n",
    "We will use the [`cwl-tes`](https://github.com/uniqueg/cwl-tes) workflow engine\n",
    "for running workflows written in the [Common Workflow Langauge\n",
    "(CWL)](https://www.commonwl.org/). `cwl-tes` extends the\n",
    "[`cwltool`](https://github.com/common-workflow-language/cwltool) CWL reference\n",
    "runner by adding a TES backend and cloud storage handlers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb7e8c5-919f-472b-be5d-dd72ead712b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running CWL workflows with `cwl-tes`\n",
    "\n",
    "For the demo, we will run the \"hash splitter\" workflow (Figure 3A), a simple\n",
    "workflow with a scatter-gather step.\n",
    "\n",
    "**Figure 3A. DAG representation of the \"hash splitter\" workflow.**\n",
    "\n",
    "![Figure 3A](images/figure_3A.svg)\n",
    "\n",
    "**Figure 3B. Service calls and data flow for running the hash splitter\n",
    "workflow.** Numbers on labels for service calls, inputs and outputs match with\n",
    "the step numbers in Figure 3A.\n",
    "\n",
    "![Figure 3B](images/figure_3B.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d161c-18c8-42cf-a419-f1fb21d4a909",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unset TASK_IDS\n",
    "INPUT_URL=\"${INPUTS[workflow]}\"\n",
    "REMOTE_STORAGE_URL=$(sed \"s|ftp://|ftp://${FTP_USER}:${FTP_PASSWORD}@|\" <<< ${FTP_INSTANCE%})\n",
    "sed \"s#{{INPUT_FILE}}#${INPUT_URL}#\" workflows/cwl_hashsplitter/hashsplitter-config.yml > .tmp\n",
    "echo \"Starting hash splitter CWL workflow on cwl-tes via TES backend\"\n",
    "echo \"TES gateway: ${TES_GATEWAY}\"\n",
    "echo \"Input data: ${INPUT_URL}\"\n",
    "TASK_IDS=$(\n",
    "    cwl-tes \\\n",
    "         --remote-storage-url $REMOTE_STORAGE_URL \\\n",
    "         --tes \"${TES%/}/ga4gh/tes\" \\\n",
    "         --user $FUNNEL_SERVER_USER \\\n",
    "         --password $FUNNEL_SERVER_PASSWORD \\\n",
    "         --timeout \"30\" \\\n",
    "         --outdir \"results/\" \\\n",
    "         --tmpdir-prefix \"results/tmp_\" \\\n",
    "         --tmp-outdir-prefix \"results/tmp-out_\" \\\n",
    "         --leave-outputs \\\n",
    "         \"workflows/cwl_hashsplitter/hashsplitter-workflow.cwl\" \\\n",
    "         \".tmp\" \\\n",
    "         2>&1 >/dev/null \\\n",
    "    | tee /dev/tty \\\n",
    "    | grep \"task id:\" \\\n",
    "    | sed -n 's/^.*task id: //p'\n",
    ")\n",
    "rm .tmp\n",
    "echo \"================================================================================\"\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533e7aef-c2e5-472a-9a54-23992be6fab9",
   "metadata": {},
   "source": [
    "> Note that it is possible that you will see `POLLING ERROR`s in the logs\n",
    "> above. This is due to an open bug in proTES. But this should happen only for\n",
    "> the first one or two rounds of status polls, and it should not affect\n",
    "> affect workflow execution.\n",
    "\n",
    "Let's check one more time the status of all tasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86b63e-e869-4e83-9358-9abf76367e3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "for TASK_ID in $TASK_IDS; do\n",
    "    echo \"Checking state of task '$TASK_ID'...\"\n",
    "    RESPONSE=$( \\\n",
    "        curl \\\n",
    "            --silent \\\n",
    "            --request \"GET\" \\\n",
    "            --header \"accept: application/json\" \\\n",
    "            --header \"Content-Type: application/json\" \\\n",
    "            --user \"${FUNNEL_SERVER_USER}:${FUNNEL_SERVER_PASSWORD}\" \\\n",
    "            \"${TES%/}/ga4gh/tes/v1/tasks/${TASK_ID}?view=BASIC\"\\\n",
    "    )\n",
    "    echo -n \"Task State: \"\n",
    "    echo $RESPONSE | jq \".state\"\n",
    "    echo -n \"Executed at: \"\n",
    "    echo $RESPONSE | jq '.logs[0].metadata.forwarded_to.url'\n",
    "    echo -n \"External task ID: \"\n",
    "    echo $RESPONSE | jq '.logs[0].metadata.forwarded_to.id'\n",
    "    echo \"================================================================================\"\n",
    "done\n",
    "echo \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655146b1-bdec-4b91-8aa6-6b5ad6e29590",
   "metadata": {},
   "source": [
    "This is all we wanted to demonstrate. But please read on for a summary of\n",
    "current limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f805294-d230-4422-b25f-b747fb39cfb0",
   "metadata": {},
   "source": [
    "## 4. Known limitations\n",
    "\n",
    "Here is a list of some limitations that users and developers should be aware\n",
    "of.\n",
    "\n",
    "- There is no way of passing storage provider credentials dynamically using TES\n",
    "  requests. And even if there was, these would potentially need to passed on\n",
    "  along a series of clients and services. For FTP access, Funnel solves this\n",
    "  by expecting URLs that contain basic auth credentials. This makes passing\n",
    "  along credentials easy, but it is perhaps not very safe, as credentials may\n",
    "  inadvertently appear in log files if clients do not handle such URLs with\n",
    "  care. TESK, on the other hand, is currently unable to process such URLs and\n",
    "  instead needs to have access to cloud storage solutions preconfigured during\n",
    "  deployment, which limits the ability to access storage solutions dynamically.\n",
    "- The different behavior of Funnel and TESK with respect to FTP/storage\n",
    "  credentials is currently limiting their interoperability, as we needed to\n",
    "  implement specific checks in proTES and in payload preparation to overcome\n",
    "  them.\n",
    "- FTP is currently the only cloud storage solution that is supported and fully\n",
    "  tested together across all used services and clients. In principle, all\n",
    "  services and clients should also have support for S3 storage, but the\n",
    "  interplay has not been fully tested, and the code supporting S3 in `cwl-tes`\n",
    "  has not been released or fully merged upstream.\n",
    "- The demo relies on a few features/changes in `py-tes` that are not part of\n",
    "  the latest release and have also not been fully merged upstream yet.\n",
    "- proTES, TESK and the `cwl-tes` and `py-tes` clients support passing an OAuth2\n",
    "  Bearer token for authorization. However, for the demo, authorization\n",
    "  requirements for these services were lifted, partly because Funnel currently\n",
    "  does not support token-based authorization (requiring basic authentication\n",
    "  instead), and partly because there are currently no interoperable access\n",
    "  control guidelines that would make this mechanism useful outside of the\n",
    "  scope of a single cloud with its own access control implementation.\n",
    "- The demo currently requires that all Funnel instances use the same\n",
    "  credentials for access control.\n",
    "- The known TES instances are currently hard-coded in the proTES gateway.\n",
    "- We have currently only tested workflow execution via the gateway with the CWL\n",
    "  \"hash splitter\" workflow. In principle, other containerized CWL workflows are\n",
    "  likely to run successfully on a TES-based compute network though. We also\n",
    "  ave conducted promising initial tests with Snakemake workflows, using the\n",
    "  Snakemake workflow engine with its TES backend. However, this has not been\n",
    "  fully tested and is therefore not a part of this demo.\n",
    "  \n",
    "**We are actively working on overcoming all of these limitations to provide\n",
    "end users and developers the ability to make use of GA4GH TES-based cloud\n",
    "environments in production settings. Please stay tuned!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
